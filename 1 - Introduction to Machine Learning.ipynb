{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "In traditional programming, create an algorithm and take in inputs to that algorithm to produce the output.\n",
    "\n",
    "In machine learning, we use the inputs and outputs together to create the algorthm.\n",
    "\n",
    "We avoid having to program possibly very complex algorithms, and instead, use statistics and other methods to do the work for us\n",
    "\n",
    "The machine learning algorithm takes in data to create the algorithm. The more data we provide, the more accurate it is. \n",
    "\n",
    "\n",
    "## What Do We Actually Do?\n",
    "We need to first understand the problem at hand.\n",
    "\n",
    "We need to then express the problem as a machine learning task\n",
    "\n",
    "Choose the appropriate Machile Learning algorithm\n",
    "\n",
    "Then evaluate the results of the algorithm\n",
    "\n",
    "## The Algorithms\n",
    "There are thousands of machine learning algorithms out there and there are hundreds more coming out every year\n",
    "\n",
    "They have three components: Representation, Evaluation and Optimization. \n",
    "\n",
    "### Representation\n",
    "How we represent the function from the input stage to the output stage\n",
    "    Decision trees\n",
    "    Logic Programs / Rules\n",
    "    Instances\n",
    "    Graphical models - Bayes/Markov Nets\n",
    "    Neural Networsk\n",
    "    Support Vector Machines\n",
    "    Model Ensembles\n",
    "    \n",
    "### Evaluation\n",
    "Given data, how do we tell if the function is good\n",
    "    Accuracy\n",
    "    Precision and Recall\n",
    "    Squared Error\n",
    "    Likelihood\n",
    "    Posterior probability\n",
    "    Cost/Utility\n",
    "    Margin\n",
    "    Entropy\n",
    "    K-L divergence\n",
    "\n",
    "### Optimization\n",
    "Given some data, how do we fidn the best function\n",
    "    Compinatorial optimization\n",
    "        Greedy Serach\n",
    "    Convex Optimization\n",
    "        Gradient Descent\n",
    "    Constrained Optimization\n",
    "        Linear Programming\n",
    "        \n",
    "## Different Types of Learning\n",
    "### Supervised (inductive) Learning\n",
    "Training data includes desired outputs\n",
    "Given examples of a function (x, f(x))\n",
    "We can predict new outputs f(x) for new x\n",
    "    In discrete cases, this is classification\n",
    "    In continuous cases, this is regression\n",
    "    f(x) = probablility(x) is a probability estimation\n",
    "\n",
    "We cover decision tree induciton, instance - based learning, neural networks, bayesian learning, logistic regression, support vector machiens, learning theory, and reinforcement learning\n",
    "\n",
    "### Unsupervised learning\n",
    "Clustering \n",
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
