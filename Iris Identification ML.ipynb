{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS Identification\n",
    "Why is IRIS all caps? Because it was BORN THIS WAY. Honestly just a typo that I'm too lazy to fix but spent more time justifying instead of fixing it. \n",
    "\n",
    "## What are we doing?\n",
    "\n",
    "We have a set of .csv files containing measurements of sepal and petal geometry. From these files, we want to predict the flower species. We do this in 5 steps\n",
    "\n",
    "1. Load the CSV files into a tensor\n",
    "2. Construct the neural network classifier\n",
    "3. Train the model using the training data\n",
    "4. Evlauate accuracy of model\n",
    "5. Classify the new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE Time\n",
    "\n",
    "## Module Import Statements\n",
    "We first need to import the necessary modules, mostly TensorFlow for our Neural Network purposes and NumPy. urllib and os are also imported to help us download the data set and save it to our directories.\n",
    "\n",
    "However, there are a bunch of weird \n",
    "\n",
    "    from __future__\n",
    "\n",
    "statements. These are kinda like import statements, only they import a specific behavior from a future release of python, in this case 3.0. They modify how absulote_import, division, and print work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Importing the Data\n",
    "We now need to import our training and testing data from the interwebs. To do this, we first need to define our file names and urls.\n",
    "\n",
    "The program automatically checks to see if the data sets already exists before downloading and saves them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "if not os.path.exists(IRIS_TRAINING):\n",
    "    raw = urllib.request.urlopen(IRIS_TRAINING_URL).read()\n",
    "    with open(IRIS_TRAINING, 'wb') as f:\n",
    "        f.write(raw)\n",
    "                \n",
    "if not os.path.exists(IRIS_TEST):\n",
    "    raw = urllib.request.urlopen(IRIS_TEST_URL).read()\n",
    "    with open(IRIS_TEST, 'wb') as f:\n",
    "        f.write(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things Changed From Tutorial\n",
    "`urllib.urlopen` does not work. Instead, we replace `import urllib` with `import urllib.request` and call `urllib.request.urlopen` on the given url. This seems to be a python 3 bug.\n",
    "\n",
    "Same thing with `write(raw)` This does now work unless you replace the `w` in `with open(XXX, 'w')` with `wb` like in `with open(XXX, 'wb')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Creating the Datasets\n",
    "Now that we have the files, we need to actually import the data into tensorflow. Good thing tensorflow has built in functions to load csv files. \n",
    "\n",
    "    filename = name of file\n",
    "    target_dtype = datatype of the output of the function\n",
    "    features_dtype = datatype of the inputs to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename = IRIS_TRAINING,\n",
    "    target_dtype = np.int,\n",
    "    features_dtype = np.float32)\n",
    "\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename = IRIS_TEST,\n",
    "    target_dtype = np.int,\n",
    "    features_dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Constructing the Deep Neural Network Classifier\n",
    "Basically, creating the brains and neurons of the thing. `tf.contrib.learn` has a bunch of prebuilt estimators that we can use. We are using the DNN classifier here. We can make it in a few lines of code. \n",
    "\n",
    "We first define our feature columns to have 4 columns for our features\n",
    "\n",
    "From there, we can define our classifier.\n",
    "\n",
    "    feature_columns = our inputs to the function.\n",
    "    hidden_units = the hidden layers and neurons in each layer\n",
    "    n_classes = how many output classes there are\n",
    "    model_dir = where to save the model checkpoints while its running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_save_summary_steps': 100, '_master': '', '_save_checkpoints_secs': 600, '_session_config': None, '_num_worker_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_model_dir': '/tmp/iris_model', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020C6823FD30>, '_task_id': 0, '_keep_checkpoint_max': 5, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_evaluation_master': '', '_environment': 'local', '_task_type': None, '_num_ps_replicas': 0, '_save_checkpoints_steps': None}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns = feature_columns,\n",
    "                                               hidden_units = [10, 20, 10],\n",
    "                                               n_classes = 3,\n",
    "                                               model_dir = \"/tmp/iris_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Define the Input Function\n",
    "We then need to create the input function as `tf.contrib.learn` uses this to grab data. In our case, our data is small, thus we can just put them in TensorFlow constants, meaning our input pipeline is very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_inputs():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Fitting the Data\n",
    "We need to now fit our classifier to the training data. This isn't very complicated. we use the `fit` method of a classifier. we pass it the input function we just generated as the input function and the number of steps to iterate over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ichis\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-2000\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into /tmp/iris_model\\model.ckpt.\n",
      "INFO:tensorflow:step = 2001, loss = 0.0443918\n",
      "INFO:tensorflow:global_step/sec: 1038.9\n",
      "INFO:tensorflow:step = 2101, loss = 0.0438516 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 958.983\n",
      "INFO:tensorflow:step = 2201, loss = 0.0433526 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 744.266\n",
      "INFO:tensorflow:step = 2301, loss = 0.0428823 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 744.286\n",
      "INFO:tensorflow:step = 2401, loss = 0.0424014 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 687.834\n",
      "INFO:tensorflow:step = 2501, loss = 0.0419769 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 906.657\n",
      "INFO:tensorflow:step = 2601, loss = 0.0415403 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 824.258\n",
      "INFO:tensorflow:step = 2701, loss = 0.0411153 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 791.541\n",
      "INFO:tensorflow:step = 2801, loss = 0.0407023 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 824.249\n",
      "INFO:tensorflow:step = 2901, loss = 0.0402992 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 898.498\n",
      "INFO:tensorflow:step = 3001, loss = 0.0399096 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 940.893\n",
      "INFO:tensorflow:step = 3101, loss = 0.0395229 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 1049.83\n",
      "INFO:tensorflow:step = 3201, loss = 0.0391468 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.996\n",
      "INFO:tensorflow:step = 3301, loss = 0.0387866 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 867.083\n",
      "INFO:tensorflow:step = 3401, loss = 0.038419 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 875.029\n",
      "INFO:tensorflow:step = 3501, loss = 0.03807 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 882.599\n",
      "INFO:tensorflow:step = 3601, loss = 0.0377233 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 845.207\n",
      "INFO:tensorflow:step = 3701, loss = 0.0373816 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 859.777\n",
      "INFO:tensorflow:step = 3801, loss = 0.0370507 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 906.675\n",
      "INFO:tensorflow:step = 3901, loss = 0.036714 (0.112 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/iris_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0365118.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'gradient_clip_norm': None, 'hidden_units': [10, 20, 10], 'optimizer': None, 'dropout': None, 'activation_fn': <function relu at 0x0000020C668F6840>, 'embedding_lr_multipliers': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x0000020C6825D5F8>, 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(input_fn = get_train_inputs, steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Model Preservation\n",
    "The classifier variable preserves the state of the model, meaning that successive training steps are cumulative. The above statement is equivalent to \n",
    "\n",
    "    classifier.fit(x=training_set.data, y = training_set.target, steps=1000)\n",
    "    classifier.fit(x = training_set.data, y = training_set.target, steps = 1000)\n",
    "\n",
    "***\n",
    "## Evaluating Model Accuracy\n",
    "We now need to evaluate the accuracy of the classifier off the test dataset. We can do this using the `evaluate` method. It takes another input function just like the `fit` method. We pass this data into the `evaluate` method which gives us the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ichis\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-27-20:44:48\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-4000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-27-20:44:49\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.966667, global_step = 4000, loss = 0.0884713\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_test_inputs():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "    return x,y\n",
    "\n",
    "accuracy_score = classifier.evaluate(input_fn = get_test_inputs,\n",
    "                                     steps = 1)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classify New Samples\n",
    "Now that we have a (pretty good, I might add) classifier developed using our data. We do this using our estimator's `predict()` method  which like `fit` and `evaluate` takes in an input function. We hard code them in here, but it is obviously trivial to give it more data.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ichis\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:347: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_classes, or set `outputs` argument.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-4000\n",
      "New Samples, Class Predictions:    [1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_samples():\n",
    "    return np.array(\n",
    "        [[6.4, 3.2, 4.5, 1.5],\n",
    "         [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "\n",
    "predictions = list(classifier.predict(input_fn = new_samples))\n",
    "\n",
    "print(\"New Samples, Class Predictions:    {}\\n\".format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
